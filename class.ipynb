{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dezin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dezin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from xml.etree import ElementTree as ET\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import RSLPStemmer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finalizado.\n",
      "Arquivo descompactado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "url = 'https://www.dropbox.com/scl/fo/2vh6qw9x2ae8zoma7md98/ALGVx_ju4WiPjneRZ68crs8?rlkey=s919cfytsov4bafkvnufmpgwg&dl=1'\n",
    "\n",
    "pathFiles = 'dados'\n",
    "fileName = 'arquivos_competicao.zip'\n",
    "filePath = os.path.join(pathFiles, fileName)\n",
    "extractPath = os.path.join(pathFiles, 'dados')\n",
    "\n",
    "if not os.path.isdir(pathFiles):\n",
    "    os.mkdir(pathFiles)\n",
    "\n",
    "\n",
    "response = requests.get(url, stream=True)\n",
    "if response.status_code == 200:\n",
    "    with open(filePath, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    print('Download finalizado.')\n",
    "else:\n",
    "    print(f'Houve um erro: {response.status_code}')\n",
    "\n",
    "\n",
    "if zipfile.is_zipfile(filePath):\n",
    "    with zipfile.ZipFile(filePath, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extractPath)\n",
    "    print('Arquivo descompactado com sucesso.')\n",
    "else:\n",
    "    print('O arquivo baixado não é um arquivo zip válido.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções auxiliares para processamento de texto\n",
    "def extract_text_from_xml(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        tree = ET.parse(file)\n",
    "        root = tree.getroot()\n",
    "        headline = root.find('.//headline')\n",
    "        headline_text = headline.text.strip() if headline is not None else ''\n",
    "        text_parts = []\n",
    "        text_section = root.find('.//text')\n",
    "        if text_section is not None:\n",
    "            for p in text_section.findall('.//p'):\n",
    "                if p.text:\n",
    "                    text_parts.append(p.text.strip())\n",
    "        return ' '.join([headline_text] + text_parts)\n",
    "    \n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\b\\w+\\b')\n",
    "    portuguese_stop_words = stopwords.words('portuguese')\n",
    "    stemmer = RSLPStemmer()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    return [stemmer.stem(token) for token in tokens if token not in portuguese_stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './news/'\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_texts = [extract_text_from_xml(os.path.join(data_path, fname)) for fname in train_df['ID']]\n",
    "train_labels = train_df['Class'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers = {\n",
    "    'TF': CountVectorizer(binary=False, stop_words=stopwords.words('portuguese')),\n",
    "    'TF-IDF': TfidfVectorizer(stop_words=stopwords.words('portuguese')),\n",
    "    'Binary': CountVectorizer(binary=True, stop_words=stopwords.words('portuguese')),\n",
    "}\n",
    "\n",
    "\n",
    "# AUC Scorer para multi-classe\n",
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    return roc_auc_score(y_test, y_pred, average=average)\n",
    "\n",
    "# Armazenar o melhor pipeline para cada vetorizador\n",
    "pipelines = {}\n",
    "\n",
    "# Testar diferentes vetorizadores e hiperparâmetros\n",
    "for key, vectorizer in vectorizers.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "    param_grid = {\n",
    "        'classifier__alpha': [0.01, 0.1, 1.0]\n",
    "    }\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=make_scorer(multiclass_roc_auc_score))\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    pipelines[key] = grid_search.best_estimator_\n",
    "    print(f\"AUC para o vetorizador {key}: {grid_search.best_score_:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_key = max(pipelines, key=lambda k: pipelines[k].score(X_train, y_train))\n",
    "best_pipeline = pipelines[best_key]\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "test_auc = multiclass_roc_auc_score(y_test, y_pred)\n",
    "print(f\"AUC no conjunto de teste para o melhor vetorizador ({best_key}): {test_auc:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
